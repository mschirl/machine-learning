{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ILV01.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFe50QQTyiE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJW4UbpDyiFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is needed later when evaluating the classifier's results.\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        cm = np.around(cm, decimals=2, out=None)  \n",
        "    \n",
        "    \n",
        "    thresh = cm.max() / 2.\n",
        "    \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klQZcm36yiFZ",
        "colab_type": "text"
      },
      "source": [
        "### ILV01.0 Read data into pandas dataframe\n",
        "\n",
        "Dataframes are two-dimensional in nature\n",
        "- organized in a row/column structure just as a spreadsheet\n",
        "- The main benefits of using Dataframes: a Dataframe can handle much larger data than most common spreadsheet software.\n",
        "\n",
        "ILV01.0 a) Specify path to the CSV containing weather data from Australia and load it into memory using pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "i7zcBHoYyiFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('./weatherAUS.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moyOiiNIyiFt",
        "colab_type": "text"
      },
      "source": [
        "ILV01.0 b) Access some rows to get an idea of the data's structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofKYcUrPyiFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use functions 'head', 'sample' or 'tail' to display data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_QZhBVMyiF7",
        "colab_type": "text"
      },
      "source": [
        "### ILV01.1 Preprocessing\n",
        "#### Try to extract useful information about the dataset using tools you already got to know during the Feature Engineering course.\n",
        "\n",
        "ILV01.1 a) How many rows/columns are in this dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiU295f1yiF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use attribute 'shape' of dataframe to access dimensions, save original shape for later use\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpu6-bDFyiGJ",
        "colab_type": "text"
      },
      "source": [
        "ILV01.1 b) Examine datatypes ('dtypes' in pandas) of the columns in your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ebGhTEyiGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use function 'info' or attribute 'dtypes'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sg52-8CyiGT",
        "colab_type": "text"
      },
      "source": [
        "ILV01.1 c) Examine statistics of the columns in your dataset. Print mean, std, min and max for each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "nHFiAFL3yiGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use function 'describe' without parameters to examine statistical characteristics of numeric columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ofoIn95FyiGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use the parameter include of the 'describe' function to print statistics of other datatypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LijsqkXvyiGm",
        "colab_type": "text"
      },
      "source": [
        "#### Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSNcvdj6yiGp",
        "colab_type": "text"
      },
      "source": [
        "ILV01.1 d) Drop columns that contain > 1/3 NaN values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EW5-OEPuyiGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Display the number of NaNs in each columns with function 'isna'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2tT7XaGgyiG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Iterate over columns in dataframe to drop each column that contains >1/3 NaNs."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOJoRgFoyiG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Display the number of NaNs in each columns with function 'isna'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwRDz_H2yiHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use attribute 'shape' of dataframe to access dimensions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV-tF8AtyiHO",
        "colab_type": "text"
      },
      "source": [
        "ILV01.1 e) Choose what to do about the remaining number of NaNs:\n",
        "\n",
        "- Option 0: Decide on a strategy to fill missing values. Be aware: columns have to be treated individually to account for numeric and non-numeric datatypes.\n",
        "- Option 1: Drop rows containing missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXVWdU6hyiHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: decide which option you want to choose.\n",
        "# 0: iterate over columns, determine if the column is numeric, and use function 'fillna' to fill in the mean of this column for each missing value.\n",
        "#    if column is non-numeric, use pandas Class 'Categorical' and its attribute 'codes' to retrieve a numerical representation of the column.\n",
        "# 1: drop each row that contains NaNs by using function 'dropna' with the correct axis\n",
        "option = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLqYxzDHyiHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Display the number of NaNs in each columns with function 'isna'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN07gk6MyiHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use attribute 'shape' of dataframe to access dimensions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUsz4v5xyiHl",
        "colab_type": "text"
      },
      "source": [
        "ILV01.1 f) Drop RISK_MM as it contains the amount of rain that was recorded on the next day in millimeters which gives away too much information for your model. Read more here: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/discussion/78316)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4baCm38RyiHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use function 'drop' with its 'columns' parameter to drop specific columns by name.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "v49kZWYDyiHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use attribute 'shape' of dataframe to access dimensions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uBKmHVvyiH1",
        "colab_type": "text"
      },
      "source": [
        "ILV01.1 g) Encode remaining non-numerical values into categories (integers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-No3ZEqyiH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use pandas Class 'Categorical' and its attribute 'codes' to retrieve a numerical representation of the column.\n",
        "# Do this only for non-numeric columns in the dataframe."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXqvQ_3qyiH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: print head & info of dataframe to check contents and datatypes of dataframe!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qEBRYWw7yiIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15VB2NCEyiIK",
        "colab_type": "text"
      },
      "source": [
        "#### Extract Labels\n",
        "\n",
        "ILV01.1 h) Construct X and Y. Where X contains all numeric columns that are possible feature candidates and Y denotes the corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7MhBsmcyiIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Construct X and Y\n",
        "\n",
        "# TODO: drop column 'RainTomorrow' after saving it in y\n",
        "\n",
        "# check how many samples there are for each class:\n",
        "print(\"# of samples: {:d}\".format(len(y)))\n",
        "print(\"Percentage of days without rain on next day: {:.2f}\".format((len(y) - np.sum(y)) / len(y)))\n",
        "print(\"Percentage of days with rain on next day: {:.2f}\".format(np.sum(y) / len(y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v__18Z1ZyiIV",
        "colab_type": "text"
      },
      "source": [
        "### ILV01.2 EDA\n",
        "\n",
        "ILV01.2 a) Draw plots of your dataset to further examine its properties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wfpZvapyiIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "sns.boxplot(data=X)\n",
        "sns.despine(offset=0,\n",
        "            trim=True)\n",
        "plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light' \n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oRWBJ7LyiIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "sns.violinplot(data=X)\n",
        "sns.despine(offset=0,\n",
        "            trim=True)\n",
        "plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light' \n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09wQN7KxyiIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = X.corr()\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(corr,\n",
        "            xticklabels=corr.columns.values,\n",
        "            yticklabels=corr.columns.values, annot=True)\n",
        "sns.despine(offset=10, trim=True)\n",
        "plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light' \n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLIf4ZRTyiIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "sns.pairplot(data=X[['MinTemp', 'Temp9am', 'Rainfall', 'Pressure9am']])\n",
        "sns.despine(offset=0,\n",
        "            trim=True)\n",
        "plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light' \n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvnBNyCGyiIs",
        "colab_type": "text"
      },
      "source": [
        "ILV01.2 b) Based on the information presented in ILV01.2 a), decide which columns to drop to further reduce dimensionality of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaSYgONPyiIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X.drop(columns=[\n",
        "    #'col_1', 'col_2', ...\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjqfFI1_yiIz",
        "colab_type": "text"
      },
      "source": [
        "### ILV01.3 Sampling\n",
        "\n",
        "ILV01.3 a) Split your dataset into a train set and test set to be able to train and evaluate a classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjwyyXeWyiI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO: Split the dataset into training and test samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNyMPsnyiI5",
        "colab_type": "text"
      },
      "source": [
        "### ILV01.4 Training & Testing\n",
        "\n",
        "ILV01.4 a) Train a Naive Bayes classifier using your train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmcJGYZOyiI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# TODO: Train a Naive Bayes classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPA5ezKyiI_",
        "colab_type": "text"
      },
      "source": [
        "ILV01.4 a) Feed the testing data into the classifier and retrieve predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP2eh0u0yiJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Predict the test data with the trained classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS2KbkPlyiJF",
        "colab_type": "text"
      },
      "source": [
        "### ILV01.5 Evaluation\n",
        "\n",
        "ILV01.5 a) Use different metrics to evaluate the training result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckFlcrGxyiJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# TODO: \n",
        "# retrieve the total number of points\n",
        "total_points = ...\n",
        "# retrieve the number of mislabeled points\n",
        "mislabeled_points = ...\n",
        "# retrieve the accuracy score of your predictions\n",
        "acc_score = ...\n",
        "\n",
        "print(\"Number of mislabeled points out of a total {:d} points: {:d}\".format(total_points, mislabeled_points))\n",
        "print(\"This results in an accuracy score of {:.2f}\".format(acc_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLhZ8IOpyiJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# TODO: Use sklearn.metrics.confusion_matrix to retrieve the confusion matrix \n",
        "# for your predictions and use the custom defined function plot_confusion_matrix(...) to print it\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7gI9iwCyiJf",
        "colab_type": "text"
      },
      "source": [
        "### ILV01.6 Back to the Drawing Board ...\n",
        "\n",
        "ILV01.6 a) Revisit EDA & Feature Engineering to further improve your result! Try different settings (e.g. scale features, drop more columns, ... )\n",
        "\n",
        "First off, let's try scaling each column with zscore by using sklearn's Standardscaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKLRlyMvyiJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
        "\n",
        "# preserve structure of data as pandas dataframe by constructing new dataframe from scaled values, columns and index of X\n",
        "X = pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns, index=X.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "X4Rtjv33yiJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "sns.boxplot(data=X)\n",
        "sns.despine(offset=0,\n",
        "            trim=True)\n",
        "plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light' \n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW1APJDhyiJs",
        "colab_type": "text"
      },
      "source": [
        "Resample, retrain and repredict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt24svoNyiJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=None, stratify=y)\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train, y_train);\n",
        "y_pred = gnb.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhPE5YlxyiJ1",
        "colab_type": "text"
      },
      "source": [
        "Reevaluate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gQF-HDmyiJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: \n",
        "# retrieve the total number of points\n",
        "total_points = ...\n",
        "# retrieve the number of mislabeled points\n",
        "mislabeled_points = ...\n",
        "# retrieve the accuracy score of your predictions\n",
        "acc_score = ...\n",
        "\n",
        "print(\"Number of mislabeled points out of a total {:d} points: {:d}\".format(total_points, mislabeled_points))\n",
        "print(\"This results in an accuracy score of {:.2f}\".format(acc_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIEKZ3LuyiKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# TODO: Use sklearn.metrics.confusion_matrix to retrieve the confusion matrix \n",
        "# for your predictions and use the custom defined function plot_confusion_matrix(...) to print it\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2FW2C9WyiKK",
        "colab_type": "text"
      },
      "source": [
        "Now, let's try dropping insignificant features (use correlation heatmap to determine possible columns to drop):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okaXZMcnyiKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = X.corr()\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(corr,\n",
        "            xticklabels=corr.columns.values,\n",
        "            yticklabels=corr.columns.values, annot=True)\n",
        "sns.despine(offset=10, trim=True)\n",
        "plt.xticks(\n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light' \n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KENaHQxlyiKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Determine which columns you want to drop and add them to the following array\n",
        "columns_to_drop = [\n",
        "    ...\n",
        "]\n",
        "\n",
        "for col in columns_to_drop:\n",
        "    if col in X.columns:\n",
        "        X = X.drop(columns=[col])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_OjgKdKyiKi",
        "colab_type": "text"
      },
      "source": [
        "Once more: resample, retrain, retest, reevaluate ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ0Vyx2SyiKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Split the dataset into training and test samples\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# TODO: Train the classifier with your data\n",
        "\n",
        "# TODO: Use the trained classifier to call the predict function on the test set, save your results to y_pred\n",
        "y_pred = ...\n",
        "\n",
        "# TODO: \n",
        "# retrieve the total number of points\n",
        "total_points = ...\n",
        "# retrieve the number of mislabeled points\n",
        "mislabeled_points = ...\n",
        "# retrieve the accuracy score of your predictions\n",
        "acc_score = ...\n",
        "\n",
        "print(\"Number of mislabeled points out of a total {:d} points: {:d}\".format(total_points, mislabeled_points))\n",
        "print(\"This results in an accuracy score of {:.2f}\".format(acc_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS-hSStlyiKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['No Rain Tomorrow', 'Rain Tomorrow']\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plot_confusion_matrix(cm, class_names,\n",
        "                          normalize=True,\n",
        "                          title='Normalized Confusion matrix',\n",
        "                          cmap=plt.cm.Blues)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQV5CkLuyiKs",
        "colab_type": "text"
      },
      "source": [
        "Now we see that there is only so much we can do when it comes to optimizing features for a simple classifier such as Naive Bayes. Since we have to deal with the imbalance of classes in this dataset (see ILV01.1 h)), let's try to optimize for minimum risk instead.\n",
        "Furthermore, let's say we want to make sure we identify rainy days with higher certainty. How would we do that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD44ZbUwyiKt",
        "colab_type": "text"
      },
      "source": [
        "### ILV01.7 Naive Bayes Minimum Risk Classifier\n",
        "\n",
        "Multiplying the probabilities put out by a Naive Bayes classifier with a cost matrix leads to a Minimum Risk classifier.\n",
        "With this type of classifier, one can optimize for distinct metrics: specificity and sensitivity (among others).\n",
        "\n",
        "According to Duda et al:\n",
        "\n",
        "Decide for $C_1$ if $(\\lambda_{21} - \\lambda_{11}) . P[C_1 | \\vec{x}]$ > $(\\lambda_{12} - \\lambda_{22}) . P[C_2 | \\vec{x}]$, otherwise decide for $C_2$\n",
        "\n",
        "Cost matrix $\\lambda$:\n",
        "$\\begin{bmatrix} \n",
        "0 & 1 \\\\\n",
        "1 & 0 \n",
        "\\end{bmatrix}$ =\n",
        "$\\begin{bmatrix} \n",
        "TP & FP \\\\\n",
        "FN & TN \n",
        "\\end{bmatrix}$=\n",
        "$\\begin{bmatrix} \n",
        "(0,0) & (0,1) \\\\\n",
        "(1,0) & (1,1) \n",
        "\\end{bmatrix}$ $\\bigg($ Duda et al.: $=\n",
        "\\begin{bmatrix} \n",
        "(1,1) & (1,2) \\\\\n",
        "(2,1) & (2,2) \n",
        "\\end{bmatrix}\\bigg)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcwIDhQGyiKu",
        "colab_type": "text"
      },
      "source": [
        "ILV01.7 a) Let's say it is of great importance to correctly predict rainy days.\n",
        "\n",
        "How would you have to design the cost matrix to achieve this goal?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85OTfDCeyiKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train);\n",
        "\n",
        "predicted_probabilities = gnb.predict_proba(X_test)\n",
        "\n",
        "cost_matrix = np.array([\n",
        "    [0, 100],\n",
        "    [1, 0]]\n",
        ")\n",
        "print('Cost Matrix:')\n",
        "print(cost_matrix)\n",
        "\n",
        "p_bayes_risk_rain = (cost_matrix[1, 0] - cost_matrix[0, 0]) * predicted_probabilities[:, 1]\n",
        "p_bayes_risk_no_rain = abs(cost_matrix[0, 1] - cost_matrix[1, 1]) * predicted_probabilities[:, 0]\n",
        "\n",
        "y_pred_minrisk = np.where(p_bayes_risk_rain > p_bayes_risk_no_rain, 1, 0)\n",
        "\n",
        "acc_minrisk = accuracy_score(y_test, y_pred_minrisk)\n",
        "\n",
        "print(\"Number of mislabeled points out of a total {:d} points: {:d}\".format(X_test.shape[0], (y_test != y_pred_minrisk).sum()))\n",
        "print(\"This results in an accuracy score of {:.2f}\".format(acc_minrisk))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEzLrcSmyiK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['No Rain Tomorrow', 'Rain Tomorrow']\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_minrisk)\n",
        "\n",
        "plot_confusion_matrix(cm, class_names,\n",
        "                          normalize=True,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P27p20myiK4",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "To conclude this first session, let's talk about the definition of specificity and sensitivity:\n",
        "\n",
        "#### Sensitivity\n",
        "Sensitivity (true positive rate (TPR)) refers to the ability to correctly predict rainy days as being rainy. A high sensitivity test is reliable when its result is negative, since it rarely predicts a day to be rainy that is not. A classifier with 100% sensitivity will recognize all rainy days in a given timeframe. Sensitivity by definition does not take into account false positives, hence it cannot be used to confidently rule out rain.\n",
        "\n",
        "#### Specifity\n",
        "Specificity (true negative rate (TNR)) measures the proportion of actual negatives that are correctly identified as such (e.g., the percentage of dry days who are correctly identified as having no rainfall). A positive result from a classifier with high specificity is useful to determine days that are certainly going to be wet. The classifier rarely gives predicts days as rainy that are going to be dry."
      ]
    }
  ]
}