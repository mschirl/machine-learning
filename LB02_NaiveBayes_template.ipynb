{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB02.0 Naive Bayes\n",
    "\n",
    "You are asked to design an automated system to detect breast cancer that uses features computed from image data displaying cell structures. In a first feasibility study, you decided to apply a Naive Bayes classifier to a dataset based on image analysis of microscopic cell material gathered with the fine needle aspiration (FDA) method.\n",
    "\n",
    "![LB02_header_image.png](resources/LB02_header_image.png?raw=true)\n",
    "\n",
    "Check the paper [\"Nuclear Feature Extraction For Breast Tumor Diagnosisâ€, SPIE, 1993](https://minds.wisconsin.edu/bitstream/handle/1793/59692/TR1131.pdf?sequence=1) for more info on the following dataset.\n",
    "\n",
    "If you need more information on the python modules you are about to use, refer to the following documentations:\n",
    "* [SciKit-Learn](https://scikit-learn.org/stable/modules/classes.html)\n",
    "* [Pandas](https://pandas.pydata.org/docs/reference/index.html#api)\n",
    "* [Seaborn](https://seaborn.pydata.org/api.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.naive_bayes import GaussianNB    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is needed later when evaluating the classifier's results.\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.around(cm, decimals=3, out=None)  \n",
    "    \n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB02.1 Explorative Data Analysis (15%)\n",
    "Before you start to train the Naive Bayes classifier, you first have to take a look at the dataset provided. The first step in each data science project revolves around cleaning, understanding and preprocessing your data.\n",
    "\n",
    "### LB02.1 a) Read data \n",
    "Specify path to the CSV containing breast cancer data and load it into memory using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = ...\n",
    "df = pd.read_csv(path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.1 b) Get an idea of the structure of the dataset\n",
    "\n",
    "* Is the dataset complete?\n",
    "* What does benign and malignant mean?\n",
    "* How many samples does each class consist of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB02.2 Sampling (and Scaling) (5%)\n",
    "After you took care of the preprocessing steps, you have to set aside a portion of the dataset which will be used as test data. It is necessary to have unseen test data in order to evaluate the ability of your classifier to generalize to unseen data.\n",
    "\n",
    "### LB02.2 a) Extract Labels\n",
    "Construct X and y. Where X contains all numeric columns that are possible feature candidates and y denotes the corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ...\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.2 b) Transform labels to categorical values\n",
    "\n",
    "In a medical setting:\n",
    "- **True positive**: Sick people correctly identified as sick\n",
    "- **False positive**: Healthy people incorrectly identified as sick\n",
    "- **True negative**: Healthy people correctly identified as healthy\n",
    "- **False negative**: Sick people incorrectly identified as healthy\n",
    "\n",
    "\n",
    "Be sure to assign 0 to each label denoted as 'benign' and 1 to each label denoted as 'malignant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.2 c) Split your dataset into a train set and test set to be able to train and evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.2 d) Feature Scaling\n",
    "If you want to make use of feature scaling, use sklearn's `StandardScaler` from the preprocessing module.\n",
    "Be sure to fit the `StandardScaler` only on the train data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB02.3 Training & Testing (10%)\n",
    "Now comes the fun part. Using sklearn, we instantiante a classifier object `GaussianNB` and use its `fit()` method to fit the classifier to the data. When that is done, calling the `predict()` function with the test set lets our classifier try its best to perform a classification on unseen data.\n",
    "\n",
    "### LB02.3 a) Train a Naive Bayes (Minimum Error) classifier using your train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.3 b) Feed the testing data into the classifier and retrieve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.3 c) Repeat LB02.3 a) and LB02.3 b) for the scaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB02.4 Evaluation (10%)\n",
    "After testing has finished, we need to evaluate the performance exhibited by our classifier. Many tools may be used here, with the confusion matrix being a simple and effective option to choose.\n",
    "\n",
    "### LB02.4 a) Accuracy and Confusion Matrix\n",
    "Compute accuracy and confusion matrix for both classifiers (scaled and not scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = ...\n",
    "acc_sc = ...\n",
    "\n",
    "print(\"Accuracy for naive Bayes minimum error classifier: {:.4f}\".format(acc))\n",
    "print(\"Accuracy for naive Bayes minimum error classifier (scaled): {:.4f}\".format(acc_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB02.5 Feature Selection (15%)\n",
    "Now that we identified two different ways to train a Naive Bayes classifier (with scaled as well as unscaled data), we want to check if the usage of feature engineering may have a positive effect on our automated breast cancer detection system.\n",
    "\n",
    "### LB02.5 a) Correlation Heatmap\n",
    "Take a look at the correlation heatmap to determine if you are able to eliminate redundant columns to reduce dimensionality without losing the discriminative power of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.5 b) Drop highly correlated features (columns)\n",
    "Drop either of the two features where correlation >= 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.5 c) Repeat LB02.3 a) and LB02.3 b) for the dataset with the selected number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.5 d) Repeat LB02.4 a) for the dataset with the selected number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sel = ...\n",
    "\n",
    "print(\"Accuracy for naive Bayes minimum error classifier: {:.4f}\".format(acc))\n",
    "print(\"Accuracy for naive Bayes minimum error classifier (scaled): {:.4f}\".format(acc_sc))\n",
    "print(\"Accuracy for naive Bayes minimum error classifier (selected): {:.4f}\".format(acc_sel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB02.6 Stratified K Fold Validation (20%)\n",
    "Now that you trained the Naive Bayes Minimum Error classifier in three different ways, you need to decide which of these classifiers produces the best result. To achieve this, you need to try to eliminate randomness to get a better idea of the performance of each model. Cross validatation is one tool that you can use to assess how the results of the classification will generalize to an independent dataset.\n",
    "\n",
    "### LB02.6 a) Classifier Comparison\n",
    "Perform a stratified 10-fold cross validation (scikit-learn: `cross_val_score`) and score for accuracy. Output the mean accuracy and its 95% confidence interval (Hint: 2-Sigma rule).\n",
    "\n",
    "Do this for all three classifiers (Naive Bayes, Naive Bayes with scaling, Naive Bayes with selected features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# Hint: You will need a pipeline object when properly applying standard scaler to the data folds used in training\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cv_folds = ...\n",
    "cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=None)\n",
    "\n",
    "print('Naive Bayes:')\n",
    "print('Mean crossfold accuracy: \\t {:.4f} (+/-{:.4f})'.format(cv_mean, ci))\n",
    "\n",
    "print('\\nNaive Bayes (scaled):')\n",
    "print('Mean crossfold accuracy: \\t {:.4f} (+/-{:.4f})'.format(cv_mean, ci))\n",
    "\n",
    "print('\\nNaive Bayes (selected features):')\n",
    "print('Mean crossfold accuracy: \\t {:.4f} (+/-{:.4f})'.format(cv_mean, ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB02.7 Naive Bayes Minimum Risk Classifier (25%)\n",
    "Since detecting cancer is of great importance to the survival chances of a patient, an automatic system detecting breast cancer needs to recognize as many potential cancer symptoms as possible.\n",
    "\n",
    "This is why scoring for accuracy might not be the best idea when deciding for a classifier to use. Instead, we need to focus on sensitivity to minimize the risk for the patients involved. With a Naive Bayes classifier, there is the possibility of multiplying the classifier's results with a cost/risk/loss matrix $\\lambda$.\n",
    "\n",
    "\n",
    "Multiplying the probabilities put out by a Naive Bayes classifier with a cost matrix leads to a Minimum Risk classifier.\n",
    "With this type of classifier, one can optimize for distinct metrics: specificity and sensitivity (among others).\n",
    "\n",
    "According to [\"Pattern Classification\", Duda et al, 2001](https://github.com/dazzz/patrec2015/blob/master/Pattern%20Classification%20by%20Richard%20O.%20Duda%2C%20David%20G.%20Stork%2C%20Peter%20E.Hart%20.pdf) (Chapter 2.2 Bayesian Decision Theory - Continuous Features):\n",
    "\n",
    "Decide for $C_1$ if $(\\lambda_{21} - \\lambda_{11}) . P[C_1 | \\vec{x}]$ > $(\\lambda_{12} - \\lambda_{22}) . P[C_2 | \\vec{x}]$, otherwise decide for $C_2$\n",
    "\n",
    "Cost matrix $\\lambda$:\n",
    "$\\begin{bmatrix} \n",
    "0 & 1 \\\\\n",
    "1 & 0 \n",
    "\\end{bmatrix}$ =\n",
    "$\\begin{bmatrix} \n",
    "TP & FP \\\\\n",
    "FN & TN \n",
    "\\end{bmatrix}$=\n",
    "$\\begin{bmatrix} \n",
    "(0,0) & (0,1) \\\\\n",
    "(1,0) & (1,1) \n",
    "\\end{bmatrix}$ $\\bigg($ Duda et al.: $=\n",
    "\\begin{bmatrix} \n",
    "(1,1) & (1,2) \\\\\n",
    "(2,1) & (2,2) \n",
    "\\end{bmatrix}\\bigg)$\n",
    "\n",
    "More information can also be found in S. Wegenkittl: Lecture on Machine Learning (Slides: 78ff)\n",
    "\n",
    "### LB02.7 a) Training and Evaluation\n",
    "Train the Naive Bayes classifier and let it predict probabilities (Hint: use `predict_proba`). Convert the Naive Bayes Minimum Error classifier into a minimum risk classifier by introducing a cost/risk/loss matrix $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the loss matrix have to look like in order to achieve the exact same result as a Naive Bayes Minimum Error Classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.7 b) Confusion Matrix\n",
    "Plot the confusion matrix for the Naive Bayes Minimum Risk classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.7 c) Sensitivity and Specificity\n",
    "\n",
    "#### Sensitivity\n",
    "Sensitivity refers to the medical test's ability to correctly detect ill patients who do have the disease. A high sensitivity test is reliable when its result is negative, since it rarely misdiagnoses those who have the disease. A test with 100% sensitivity will recognize all patients with the disease by testing positive. Sensitivity by definition does not take into account false positives, hence it cannot be used for ruling out a disease.\n",
    "\n",
    "$Sensitivity = TPR = \\frac{TP}{TP+FN}$\n",
    "\n",
    "#### Specifity\n",
    "Specificity relates to the test's ability to correctly reject healthy patients without a disease. Specificity of a test is the proportion of healthy patients known not to have the disease, who will test negative for it. A positive result in a test with high specificity is useful for ruling a disease. The test rarely gives positive results in healthy patients.\n",
    "\n",
    "$Specificity = TNR = \\frac{TN}{TN+FP}$\n",
    "\n",
    "Print accuracy, sensitivity and specificity of the minimum risk classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_minrisk = ...\n",
    "sensitivity = ...\n",
    "specificity = ...\n",
    "\n",
    "print(\"\\nAccuracy for naive Bayes minimum risk classifier: {:.4f}\".format(acc_minrisk))\n",
    "print(\"Sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"Specifity: {:.4f}\".format(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB02.7 d) Sensitivity Optimization\n",
    "\n",
    "Change the values of the `cost_matrix` in LB02.7 a) to achieve a sensitivity of > 0.98.\n",
    "Pay attention and try to understand how the values of accuracy, sensitivity and specificity correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
