{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.0 The Perceptron\n",
    "\n",
    "In this lecture we are going to learn the basics of a standard artificial neural network (ANN). \n",
    "\n",
    "<img src=\"resources/LB03_perceptron.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "* The perceptron is the smallest computational unit in an artificial neural network. \n",
    "* It takes inputs $x_1, ..., x_d$ (features of the data set)\n",
    "* Computes the activation: $act = \\sum_{i=1}^{d} w_i \\cdot x_i + w_0$\n",
    "* The parameters of one perceptron are weights $w_i$ and bias $w_0$\n",
    "* The output of one perceptron is $o = f(act)$ where $f()$ is a non-linear transfer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need this in order to get interactive plots\n",
    "%matplotlib inline  \n",
    "%matplotlib notebook\n",
    "%pylab\n",
    "\n",
    "# Importing the packages needed for this lecture\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.1 Perceptron definition (50%)\n",
    "In this task, you will define the perceptron as a python class - with all its important attributes and methods.\n",
    "\n",
    "## LB03.1 a) Non linear transfer function\n",
    "As already stated in LB03.0, every perceptron needs a non-linear transfer function. Define a python function that calculates:\n",
    "\n",
    "<center>$\\large logistic(x) = \\frac{1}{1 + e^{-x}}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    # TODO: Return the result of the calculation\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.1 b) Perceptron class\n",
    "Define a simple perceptron class which takes 2 arguments in the constructor: \n",
    "* `input_dim` (input dimensionality)\n",
    "* `lr` (learning rate). \n",
    "\n",
    "The class should also implement the functions `forward()` and `backward()`.\n",
    "\n",
    "#### `forward(inputs)`:\n",
    "The function `forward()` should take the inputs (which are the features of the data sample) and compute the output of the perceptron. In order to calculate the output you will need to calculate the sum of the multiplication of weights with the inputs. Afterwards the result of the activation is going through a non-linear function.\n",
    "\n",
    "$\\normalsize  act = \\sum_{i=1}^{d} w_i \\cdot x_i + w_0$\n",
    "\n",
    "$\\normalsize o = f(act)$ where $o$ is output\n",
    "\n",
    "#### `backward(inputs, label, output)`:\n",
    "This function is taking `inputs`, `label` and `output` as an argument and updates the parameters of the perceptron using the computed gradient. \n",
    "\n",
    "##### Calculating the gradient: \n",
    "\n",
    "\n",
    "$\\large \\frac{\\partial loss(x_1, x_2, \\tau)}{w_I} = (o - \\tau) \\cdot (logistic(act) \\cdot (1 - logistic(act)) \\cdot x_I$ \n",
    "\n",
    "More information - S. Wegenkittl: Lecture on Machine Learning (Slides: 106ff)\n",
    "\n",
    "##### Basic backpropagation: \n",
    "\n",
    "$\\large par_1 = par_0 - \\alpha_0 \\cdot grad_0(loss)$ \n",
    "\n",
    "\n",
    "More information - S. Wegenkittl: Lecture on Machine Learning (Slides: 101ff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, input_dim, lr=0.01):\n",
    "        # TODO: Initialize the learning rate\n",
    "        self.lr = ...\n",
    "        # TODO: Initialize the weights of the perceptron with uniformly distributed random values between -1 and 1\n",
    "        # Every single input (feature) of your perceptron has an associated weight, thus\n",
    "        # the shape of the variable `weights` should be (input_dim, )\n",
    "        self.weights = ...\n",
    "        # TODO: Initialize the bias of the perceptron with a uniformly distributed random value between -1 and 1\n",
    "        # Every perceptron in the network has a bias associated to it, e.g. the bias should be a scalar value\n",
    "        self.bias = ...\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # TODO: Compute the activation of the perceptron\n",
    "        act = ...\n",
    "        # TODO: Compute the output using your logistic function previously defined\n",
    "        output = ...\n",
    "        return output\n",
    "\n",
    "    def backward(self, inputs, label, output):\n",
    "        # TODO: Compute the loss according to S. Wegenkittl: Lecture on Machine Learning slide 106 \n",
    "        loss = ...\n",
    "        # TODO: Compute the gradient of the loss\n",
    "        gradient = ...\n",
    "        # TODO: Update the weights and the bias using the computed gradient and defined learning rate\n",
    "        self.weights = ...\n",
    "        self.bias = ...\n",
    "        # TODO: Return the calculated loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.1 c) Training your perceptron\n",
    "Use the functions `forward()` and `backward()` of the perceptron class in order to train and adjust the parameters of this simple network. The function `train()` will take `perceptron` which is the only building block of your simple network, `X` input data, `y` labels and `epochs` as its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(perceptron, X, y, epochs=100):\n",
    "    # TODO: Define an empty array which will contain your losses over the epochs\n",
    "    loss = ...\n",
    "    # TODO: Repeat the training process epochs times\n",
    "    for ...:\n",
    "        # TODO: Loop through your training data and use the function forward() \n",
    "        # to get the output of the perceptron using its current parameters and the function \n",
    "        # backward() in order to adjust the weights according to the current gradient of the loss\n",
    "        for ...:\n",
    "            \n",
    "        # TODO: Calculate the average loss over all samples in one epoch\n",
    "        loss.append(...)\n",
    "    # TODO: Return losses\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.1 d) Prediction\n",
    "Use the function `forward()` of the perceptron class to make the prediction based on the input data. Because we use the logistic function as our output transfer function, `predict()` returns a percentage value that indicates the probability whether a sample is part of the <em>positive</em> class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(perceptron, X):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.1 d) Plotting the loss curve\n",
    "Define a function `plot_loss` which will take an array of losses `loss` over the epochs as an argument and plots losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.2 Experiment using a self implemented perceptron (30%)\n",
    "Now that the perceptron has been defined, we are going to use it in a classification task of two very simple data sets.\n",
    "\n",
    "## LB03.2 a) 2d data set\n",
    "Start with a simple 2d data set. Create `X` which consists of $x_1$ and $x_2$ and corresponding label `y` which consists of $y$ from the following table:\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ |\n",
    "| :-: | :-: | :-: |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 1 |\n",
    "\n",
    "This data set will be used for the training of your perceptron. To be able to evaluate if our perceptron was able to complete its task, you should take a look at the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the X and y\n",
    "X = np.array([\n",
    "    ...\n",
    "])\n",
    "y = np.array([...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an object of class Perceptron with the wished dimensionality\n",
    "perceptron = ...\n",
    "\n",
    "# TODO: Train the created perceptron using the input data and labels for 1000 epochs\n",
    "losses = ...\n",
    "\n",
    "# TODO: Plot your losses over the epochs\n",
    "plot_loss(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now you will use the predict() function\n",
    "# For this simple case, we will use the same data we trained the classificator with \n",
    "# for the prediction\n",
    "predict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets visualize the data and the decision boundary in a 2d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a matplotlib figure\n",
    "fig = plt.figure()\n",
    "# Using the axes() function to access the ax object\n",
    "ax = plt.axes()\n",
    "\n",
    "# Using the scatter() function to plot the data points in the figure\n",
    "ax.scatter(X[:, 0], X[:, 1], c = y, cmap=\"Spectral\");\n",
    "\n",
    "# Creating the x values of the line\n",
    "xx = np.linspace(-2,2,500)\n",
    "# Calculating the corresponding y value\n",
    "yy = -((perceptron.weights[0]*xx) + perceptron.bias)/perceptron.weights[1]\n",
    "\n",
    "# Limiting the axes\n",
    "plt.xlim((-0.5, 1.5))\n",
    "plt.ylim((-0.5, 1.5))\n",
    "\n",
    "# Plotting the decision boundary\n",
    "ax.plot(xx, yy, 'g-', linewidth=1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.2 b) 3d data set\n",
    "Start with a simple 3d data set. Create `X` which consists of $x_1$, $x_2$ and $x_3$ and corresponding label `y` which consists of $y$ from the following table:\n",
    "\n",
    "| $x_1$ | $x_2$ | $x_3$ | $y$ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 1 |\n",
    "| 1 | 0 | 1 | 1 |\n",
    "| 1 | 1 | 1 | 0 |\n",
    "\n",
    "Don't forget to train your perceptron and evaluate its learning progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Construct the X and y\n",
    "X = np.array([\n",
    "    ...\n",
    "])\n",
    "y = np.array([...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a new perceptron object with given dimensionality\n",
    "perceptron_3d = ...\n",
    "\n",
    "# TODO: Train the newly created perceptron with 3d data for 1000 epochs\n",
    "losses = ...\n",
    "\n",
    "# TODO: Plot your losses over the epochs\n",
    "plot_loss(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the predict() function to obtain the associated labels\n",
    "# For this simple case, we will use the same data we trained the classificator with \n",
    "# for the prediction\n",
    "predict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Did you get the expected results? Describe what you see.\n",
    "(Hint: Think about the properties of a single perceptron and its abilities when separating a feature space. The plot in the cell below should also give you more information on this question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets visualize the data and the decision boundary in a 3d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a matplotlib figure\n",
    "fig = plt.figure()\n",
    "# Using the axes() function to access the ax object\n",
    "ax = plt.axes(projection = \"3d\")\n",
    "\n",
    "# Creating the x range\n",
    "xx = np.linspace(-0.5, 1.5, 50)\n",
    "# Creating the y range\n",
    "yy = np.linspace(-0.5, 1.5, 50)\n",
    "\n",
    "# Creating meshgrid which is going to be used for the calculation \n",
    "# of the Z values of corresponding hyperplane\n",
    "xx, yy = np.meshgrid(xx, yy)\n",
    "\n",
    "# Calculating the Z values of the hyperplane using the weights of the perceptron\n",
    "Z = -(perceptron_3d.weights[0]*xx + perceptron_3d.weights[1]*yy + perceptron_3d.bias)/perceptron_3d.weights[2]\n",
    "\n",
    "# Limiting the axes\n",
    "plt.xlim((-0.5, 1.5))\n",
    "plt.ylim((-0.5, 1.5))\n",
    "\n",
    "# Plotting the decision boundary hyperplane \n",
    "ax.plot_surface(xx, yy, Z, alpha=0.5)\n",
    "\n",
    "# Using the scatter3D() function to plot the data points in the figure\n",
    "ax.scatter3D(X[:, 0], X[:, 1], X[:, 2], c = y, cmap=\"Spectral\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.3 Experiment with a more complex artificial neural network (20%)\n",
    "\n",
    "Because implementing more complex neural networks is not a task you would typically do in a jupyter notebook, we are going to rely on the `Keras` framework to do this for us.\n",
    "To see if we can solve the 3d classification challenge from LB03.2, we will leave the inputs and labels the same, but use a more complex neural network (with hidden layers).\n",
    "\n",
    "| $x_1$ | $x_2$ | $x_3$ | $y$ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 1 |\n",
    "| 1 | 0 | 1 | 1 |\n",
    "| 1 | 1 | 1 | 0 |\n",
    "\n",
    "For more information on the `Keras` libary refer to https://keras.io/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Construct the X and y\n",
    "X = np.array([\n",
    "    ...\n",
    "])\n",
    "y = np.array([...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.3 a) Construct a simple neural network\n",
    "\n",
    "In this task you will construct a simple neural network with two layers. The number of nodes are to be decided on. How many nodes do you need in order to successfully classify this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the number of the nodes in the hidden layer\n",
    "hidden_nodes = ...\n",
    "# TODO: Define the dimension of the input data\n",
    "input_dimension = ...\n",
    "# TODO: Define the dimensionality of the output\n",
    "output_dimension = ...\n",
    "# TODO: Choose an appropriate transfer function for your network - https://keras.io/activations/\n",
    "transfer_function = ...\n",
    "\n",
    "# Creating a sequential model and adding two layers to the model\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_nodes, input_dim=input_dimension, activation=transfer_function))\n",
    "model.add(Dense(output_dimension, activation=transfer_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the least number of nodes in the hidden layer which is able to solve this classification task? Why do we care so much about the number of nodes in the hidden layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.3 b) Model compiling\n",
    "Before you can start training your model it is necessary to compile the model with whished loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose an appropriate loss function for this problem - https://keras.io/losses/\n",
    "loss_function = ...\n",
    "\n",
    "# TODO: Choose an optimizer - https://keras.io/optimizers/ and S. Wegenkittl: Lecture on Machine Learning slide 116\n",
    "optimizer = ...\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.3 c) Fitting the model\n",
    "It is time to train the compiled model. Use the function `fit()` with appropriate arguments to train your model for 5000 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the model for 5000 epochs\n",
    "history = model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the plot_loss() function you previously defined to plot the loss of the fitted model\n",
    "losses = ...\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03.3 d) Evaluation\n",
    "You will now use the function `predict()` with appropriate arguments to obtain the labels of the data from the model. For this simple case you will use the training data for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the data\n",
    "model.predict(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
