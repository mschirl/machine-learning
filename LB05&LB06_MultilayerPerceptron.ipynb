{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LB05 & LB06 Multilayer Perceptron\n",
    "\n",
    "In this lecture, you are going to apply the knowledge you acquired during this course. Your goal is to fully configure, train and evaluate a Multilayer Perceptron that classifies images of handwritten digits from the [MNIST](http://yann.lecun.com/exdb/mnist/) database.\n",
    "\n",
    "In the link provided, you find a detailed description of the dataset as well as a table containing error rates of classifiers that were previously trained by other researchers.\n",
    "\n",
    "<center><img src=\"resources/LB05_MNIST.png\" style=\"width: 75%\"/></center>\n",
    "<center><a href=\"https://towardsdatascience.com/improving-accuracy-on-mnist-using-data-augmentation-b5c38eb5a903\">towardsdatascience.com</a></center>\n",
    "\n",
    "\n",
    "The images in the large [MNIST](http://yann.lecun.com/exdb/mnist/) dataset are 28x28 pixels. To simplify the task a bit, we will use downscaled images (8x8).\n",
    "\n",
    "We challenge you to build a classifier based on the concept of Multilayer Perceptron that comes close to the error rates stated in the aforementioned table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, learning_curve, StratifiedKFold\n",
    "import os, keras\n",
    "from sklearn.datasets import load_digits\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html\n",
    "# we adapted the code to introduce our own scoring (using binary-crossentropy loss) specifically for keras models\n",
    "\n",
    "def custom_learning_curve(model, X, y, n_folds = 10, train_sizes = np.linspace(0.1, 1, 10)):\n",
    "    \"\"\"\n",
    "    Learning curve.\n",
    "    Determines cross-validated training and test scores for different training\n",
    "    set sizes.\n",
    "    \n",
    "    A cross-validation generator splits the whole dataset k times in training\n",
    "    and test data. Subsets of the training set with varying sizes will be used\n",
    "    to train the estimator and a score for each training subset size and the\n",
    "    test set will be computed. Afterwards, the scores will be averaged over\n",
    "    all k runs for each training subset size.\n",
    "    \n",
    "    Returns (adapted by reufko & mschirl):\n",
    "    -------\n",
    "    train_counts : array of shape (train_sizes,) holding the average number of samples used to train the model\n",
    "\n",
    "    train_losses : array of shape (n_folds, train_sizes) holding the train losses computed for each fold and train_size\n",
    "\n",
    "    valid_losses : array of shape (n_folds, train_sizes) holding the validation losses computed for each fold and train_size \n",
    "    \"\"\"\n",
    "    \n",
    "    model.save_weights('initial_weights.h5')\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=None)\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_counts = []\n",
    "    \n",
    "    for train, valid in kfold.split(X, y.argmax(1)):\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        train_count = []\n",
    "        for percentage in train_sizes:\n",
    "            if percentage == 1.0:\n",
    "                X_train, y_train = (X[train, :], y[train])\n",
    "            else:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X[train, :], y[train], test_size=1-percentage, random_state = None)\n",
    "            history = model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "            train_loss.append(model.evaluate(X_train, y_train, verbose=0)[0])\n",
    "            valid_loss.append(model.evaluate(X[valid, :], y[valid], verbose=0)[0])\n",
    "            model.load_weights('initial_weights.h5')\n",
    "            train_count.append(X_train.shape[0])\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        train_counts.append(train_count)\n",
    "    os.remove(\"initial_weights.h5\")\n",
    "    return np.array(train_counts).T.mean(axis=1), np.array(train_losses).T, np.array(valid_losses).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the index of an epoch where early stopping (based patience and epsilon) should be performed.\n",
    "def get_early_stopping_index(history, patience=5, epsi = 0.0001):\n",
    "  counter = 0\n",
    "  max_loss = np.array(history[\"val_loss\"]).max()\n",
    "  i=0\n",
    "\n",
    "  for index, row in enumerate(history[\"val_loss\"]):\n",
    "    if(counter==patience):\n",
    "      break\n",
    "\n",
    "    if(row<max_loss-epsi):\n",
    "      i=index\n",
    "      max_loss=row\n",
    "      counter = 0\n",
    "    else:\n",
    "      counter+=1\n",
    "      \n",
    "  return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5), scoring=None, title=\"\"):\n",
    "    # Function taken from\n",
    "    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    train_sizes, train_scores, test_scores = custom_learning_curve(estimator, X, y, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training loss\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation loss\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function that plots loss over epochs and accuracy over epochs (for both train and validation set).\n",
    "# Also adds a marker for the epoch, where early stopping should be performed if the appropriate index is passed\n",
    "def plot_learning_curve_over_epochs(history = None, early_stopping = None):\n",
    "    fig = plt.figure(figsize=(12, 10), dpi=80)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train Loss', 'Validation Loss'], loc='best')\n",
    "    if early_stopping:\n",
    "        plt.axvline(x=early_stopping+1, color='r', linestyle='-', label=\"Early Stopping: \"+str(early_stopping+1))\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train Accuracy', 'Validation Accuracy'], loc='best')\n",
    "    if early_stopping:\n",
    "        plt.axvline(x=early_stopping+1, color='r', linestyle='-', label=\"Early Stopping: \"+str(early_stopping+1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB05 Full configuration, training and evaluation of a MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of this section, we already the necessary data loading functions for you. After that, you will be on your own.\n",
    "\n",
    "You may need the official documentation of [Keras](https://keras.io/api/) and [tensorflow](https://www.tensorflow.org/versions)  to complete the tasks below. This is why the cell below will print your current Keras and tensorflow version for you, to make it easier for you to find matching documentation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: 2.3.1\n",
      "tensorflow: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import keras as k\n",
    "import tensorflow as tf\n",
    "print('Keras: {:s}'.format(k.__version__))\n",
    "print('tensorflow: {:s}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.1 Loading and preprocessing the MNIST dataset\n",
    "The data needed for this excercise is being loaded and plotted in the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 1797\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the input dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# print sample size\n",
    "sample_size = len(X)\n",
    "print(\"Number of Samples: %i\" % sample_size)\n",
    "\n",
    "# display the first digits\n",
    "num_digits = 10\n",
    "digits_display = digits.images[0:num_digits]\n",
    "\n",
    "for i,image in enumerate(digits_display):\n",
    "    plt.subplot(1, num_digits, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.1 a) Data Preparation\n",
    "Now that you have your data set, perform all necessary steps to be able to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.2 Configure a baseline ANN\n",
    "\n",
    "After loading and preparing the dataset, you are now ready to configure, train and evaluate your MLP.\n",
    "The first configuration will (most likely) lead to a baseline neural network that you will try to improve in later tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.2 a) Model creation\n",
    "Define the following:\n",
    "- input dimensionality, \n",
    "- output dimensionality, \n",
    "- number of layers you want to use (**be sure to use 3 hidden layers at most**), \n",
    "- number of nodes for each layer,\n",
    "- transfer functions of the hidden layers and the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.2 b) Model compilation\n",
    "- Be sure to define an optimizer & an appropriate loss function.\n",
    "- Compile and plot the number of trainable parameters (`model.summary()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.2 c) Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the created model with an appropriate number of epochs and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.2 d) Evaluation\n",
    "- Evaluate the model's training progress using the learning curve functions provided.\n",
    "\n",
    "- Be aware of the fact, that since this model's complexity is higher than the complexity of the models we trained in the last classes, the function `plot_learning_curve()` will take some time to run. You can stick with the `plot_learning_curve_over_epochs()` during the lesson and use the other function if you have enough time at home.\n",
    "- Plot a confusion matrix as well as the overall accuracy. Also document precision and recall for each class. If you want to be able to compare your model's performance to the classifiers in the [MNIST](http://yann.lecun.com/exdb/mnist/) table, you may want to calculate the error rate too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.3 Change the model capacity of your baseline ANN\n",
    "This task aims to help you find an appropriate model architecture that is neither too complex nor too simple (bias vs. variance tradeoff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.3 a) Choosing appropriate model complexity\n",
    "- Add or remove hidden layers, change the number of neurons for each hidden layer.\n",
    "- Leave the rest of the parameters the same as in the baseline.\n",
    "- Calculate the number of trainable parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.3 b) Training and evaluation\n",
    "Repeat the steps from LB05.2 c) and LB05.2 d) for the newly created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.4 Experiment with optimizers, learning rates and epochs as well as batch sizes\n",
    "\n",
    "Now that you found a model complexity that is neither too high nor too simple, you are going to try out different optimizers and optimizers. In a later step, you will alter the training time as well as the number of samples that are contained in a single batch to further improve training performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.4 a) Optimizer\n",
    "Try out different optimizers for this problem. Which one works best for you? Document your results by plotting learning curves and evaluating your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.4 b) Learning rates\n",
    "Once you found an appropriate optimizer, try different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens to the learning curve of your model when high (<=0.1) and low (>$10^{-8}$) learning rates are chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.4 c) Epochs and batch sizes\n",
    "Find a combination of epochs and batch size that is suitable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if the batch size is larger than the number of samples in `X_train`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.4 d) Training and evaluation\n",
    "Repeat the steps from LB05.2 c) and LB05.2 d) for the newly created model. Document your findings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.5 Improve your model's generalization capabilities (by preventing overfitting)\n",
    "As a last step that aims to improve your model, you are going to introduce regularization as well as early stopping. These techniques may help you to prevent potential overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.5 a) Regularization\n",
    "Add regularization to your hidden layers. The type of regularization and the weights you regularize are up to you. Experiment with different values for the regularization parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if you choose high values for $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.5 b) Early Stopping\n",
    "Finally, add the `EarlyStopping` callback to your model's `fit()` method with approriate values for epsilon and patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is there a default value for epsilon that you may stick to when using `EarlyStopping`?  Also, explain the relationship between loss function, patience and epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.6 Final evaluation\n",
    "The optimizations in LB05.5 should help to improve your model's ability to learn a generalization of its input dataset. Repeat the steps from LB05.2 c) and LB05.2 d) for the newly created model to verify this hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB05.6 a) Were you successful in improving your model when compared to the baseline? Document your results.\n",
    "\n",
    "Depending on your result, you may also want to compare your model's performance to the classifiers listed in the [MNIST](http://yann.lecun.com/exdb/mnist/) table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB06 Questions\n",
    "\n",
    "To conclude this lession, we want you to answer a few questions that may also help you when preparing for your exam in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** You printed the number of trainable parameters in earlier tasks. What are the trainable parameters in a multilayer perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** What is the difference between gradient descent and stochastic gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** What properties does \"Adam\" combine and how is it different from SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** What is the purpose of \"momentum\" in gradient based optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** What is a possible consequence if an insufficient number of neurons is chosen in your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** What is a possible consequence if an excessive number of neurons is chosen in your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Why do we care about regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8:** What is the \"vanishing gradient\" problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
